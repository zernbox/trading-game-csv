{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "(1) finished after 1550 timesteps, total profit: -378.10766847989504, memery: 0\n",
      "(2) finished after 1191 timesteps, total profit: -354.85142172681975, memery: 0\n",
      "(3) finished after 34281 timesteps, total profit: -366.41246353579146, memery: 5987\n",
      "(4) finished after 1394 timesteps, total profit: -391.2549554443455, memery: 5991\n",
      "(5) finished after 1228 timesteps, total profit: -414.93931079121955, memery: 6003\n",
      "(6) finished after 1394 timesteps, total profit: -371.296534852914, memery: 6003\n",
      "(7) finished after 1367 timesteps, total profit: -351.2323108470363, memery: 6006\n",
      "(8) finished after 1218 timesteps, total profit: -416.001367591529, memery: 6007\n",
      "(9) finished after 1325 timesteps, total profit: -361.46909099535907, memery: 6007\n",
      "(10) finished after 1209 timesteps, total profit: -364.9035138400277, memery: 6007\n",
      "(11) finished after 875 timesteps, total profit: -350.61460520403375, memery: 6008\n",
      "(12) finished after 1425 timesteps, total profit: -362.7774257489806, memery: 6012\n",
      "(13) finished after 1502 timesteps, total profit: -374.8483818125951, memery: 6026\n",
      "(14) finished after 913 timesteps, total profit: -354.9541906125557, memery: 6026\n",
      "(15) finished after 1323 timesteps, total profit: -350.18853825887163, memery: 6026\n",
      "(16) finished after 1201 timesteps, total profit: -351.80650222329444, memery: 6034\n",
      "(17) finished after 1383 timesteps, total profit: -358.70918242025004, memery: 6044\n",
      "(18) finished after 1342 timesteps, total profit: -351.5310766330932, memery: 6045\n",
      "(19) finished after 926 timesteps, total profit: -365.5368865835663, memery: 6046\n",
      "(20) finished after 874 timesteps, total profit: -351.0447673900389, memery: 6046\n",
      "(21) finished after 1506 timesteps, total profit: -353.0861325386993, memery: 6046\n",
      "(22) finished after 1414 timesteps, total profit: -370.7200343005801, memery: 6048\n",
      "(23) finished after 1209 timesteps, total profit: -350.3687885203425, memery: 6049\n",
      "(24) finished after 1481 timesteps, total profit: -366.49749686516145, memery: 6050\n",
      "(25) finished after 1424 timesteps, total profit: -358.9172843785418, memery: 6053\n",
      "(26) finished after 1333 timesteps, total profit: -367.4527291839887, memery: 6055\n",
      "(27) finished after 1317 timesteps, total profit: -372.66214592977184, memery: 6056\n",
      "(28) finished after 697 timesteps, total profit: -354.9441247293006, memery: 6058\n",
      "(29) finished after 1373 timesteps, total profit: -361.3522458998508, memery: 6058\n",
      "(30) finished after 1411 timesteps, total profit: -370.06343157302746, memery: 6059\n",
      "(31) finished after 1391 timesteps, total profit: -365.5961244228788, memery: 6061\n",
      "(32) finished after 18718 timesteps, total profit: -867.6037017293763, memery: 8203\n",
      "(33) finished after 7897 timesteps, total profit: -386.51352039785274, memery: 9031\n",
      "(34) finished after 1715 timesteps, total profit: -371.67025859290663, memery: 9031\n",
      "(35) finished after 1512 timesteps, total profit: -381.3371654150445, memery: 9031\n",
      "(36) finished after 4114 timesteps, total profit: -350.97066109151774, memery: 9277\n",
      "(37) finished after 7611 timesteps, total profit: -749.6860883742411, memery: 10071\n",
      "(38) finished after 1806 timesteps, total profit: -361.11683586630625, memery: 10094\n",
      "(39) finished after 4575 timesteps, total profit: -383.73516892290127, memery: 10120\n",
      "(40) finished after 1266 timesteps, total profit: -372.5036652921186, memery: 10120\n",
      "(41) finished after 4538 timesteps, total profit: -371.6444183057317, memery: 10168\n",
      "(42) finished after 1505 timesteps, total profit: -378.6173056463747, memery: 10170\n",
      "(43) finished after 1341 timesteps, total profit: -354.9004876392998, memery: 10170\n",
      "(44) finished after 1443 timesteps, total profit: -352.32794994700765, memery: 10170\n",
      "(45) finished after 1613 timesteps, total profit: -360.9715093825827, memery: 10173\n",
      "(46) finished after 4098 timesteps, total profit: -411.4958349506827, memery: 10239\n",
      "(47) finished after 14951 timesteps, total profit: -418.86270603212324, memery: 14295\n",
      "(48) finished after 1413 timesteps, total profit: -365.97425671772106, memery: 14297\n",
      "(49) finished after 1397 timesteps, total profit: -385.5321546991014, memery: 14298\n",
      "(50) finished after 1506 timesteps, total profit: -371.13701450149716, memery: 14300\n",
      "(51) finished after 1388 timesteps, total profit: -376.41924976735356, memery: 14301\n",
      "(52) finished after 1572 timesteps, total profit: -391.01888821864895, memery: 14304\n",
      "(53) finished after 1359 timesteps, total profit: -395.1359057464113, memery: 14307\n",
      "(54) finished after 1322 timesteps, total profit: -355.4608385308931, memery: 14307\n",
      "(55) finished after 1336 timesteps, total profit: -368.16628241098664, memery: 14308\n",
      "(56) finished after 1452 timesteps, total profit: -394.31107967279627, memery: 14311\n",
      "(57) finished after 1461 timesteps, total profit: -369.67250591204305, memery: 14314\n",
      "(58) finished after 1470 timesteps, total profit: -369.52947089111785, memery: 14314\n",
      "(59) finished after 1445 timesteps, total profit: -358.44764860090714, memery: 14316\n",
      "(60) finished after 1654 timesteps, total profit: -359.7862851147266, memery: 14316\n",
      "(61) finished after 1742 timesteps, total profit: -370.9192320740457, memery: 14316\n",
      "(62) finished after 1403 timesteps, total profit: -351.87903041074924, memery: 14316\n",
      "(63) finished after 1790 timesteps, total profit: -356.8043116965153, memery: 14317\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "#matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    if(exchange.floating_profit > 0):\n",
    "        reward = exchange.floating_profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    else:\n",
    "        reward = exchange.profit * 0.001\n",
    "    \n",
    "    \n",
    "    reward = exchange.floating_profit * (exchange.info[\"index\"] - 50) * 0.01\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    #print(reward)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 1000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.fc2 = nn.Linear(50, 50).to(device)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, 50).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"floating\"] > 20):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if(done):\n",
    "            #torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
