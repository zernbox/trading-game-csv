{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    return history.obs_list\n",
    "\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "done = False\n",
    "rewards = 0\n",
    "obs = env.reset()\n",
    "for i in range(101):\n",
    "    action = random.sample([0, 1, 2], 1)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    #env.render()\n",
    "    #rewards += reward\n",
    "\n",
    "    if(i%100 == 0):\n",
    "        print('{}K ({})'.format(i+1,reward))\n",
    "\n",
    "        print(obs.shape[0])\n",
    "      \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 輸入層 (state) 到隱藏層，隱藏層到輸出層 (action)\n",
    "        self.fc1 = nn.Linear(n_states, n_hidden)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(n_hidden, n_actions)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x) # ReLU activation\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self, n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity):\n",
    "        self.eval_net, self.target_net = Net(n_states, n_actions, n_hidden), Net(n_states, n_actions, n_hidden)\n",
    "\n",
    "        self.memory = np.zeros((memory_capacity, n_states * 2 + 2)) # 每個 memory 中的 experience 大小為 (state + next state + reward + action)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0 # 讓 target network 知道什麼時候要更新\n",
    "\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.target_replace_iter = target_replace_iter\n",
    "        self.memory_capacity = memory_capacity\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if np.random.uniform() < self.epsilon: # 隨機\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else: # 根據現有 policy 做最好的選擇\n",
    "            actions_value = self.eval_net.forward(x) # 以現有 eval net 得出各個 action 的分數\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()[0] # 挑選最高分的 action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # 打包 experience\n",
    "        transition = np.hstack((state, [action, reward], next_state))\n",
    "\n",
    "        # 存進 memory；舊 memory 可能會被覆蓋\n",
    "        index = self.memory_counter % self.memory_capacity\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # 隨機取樣 batch_size 個 experience\n",
    "        sample_index = np.random.choice(self.memory_capacity, self.batch_size)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_state = torch.FloatTensor(b_memory[:, :self.n_states])\n",
    "        b_action = torch.LongTensor(b_memory[:, self.n_states:self.n_states+1].astype(int))\n",
    "        b_reward = torch.FloatTensor(b_memory[:, self.n_states+1:self.n_states+2])\n",
    "        b_next_state = torch.FloatTensor(b_memory[:, -self.n_states:])\n",
    "\n",
    "        # 計算現有 eval net 和 target net 得出 Q value 的落差\n",
    "        q_eval = self.eval_net(b_state).gather(1, b_action) # 重新計算這些 experience 當下 eval net 所得出的 Q value\n",
    "        q_next = self.target_net(b_next_state).detach() # detach 才不會訓練到 target net\n",
    "        q_target = b_reward + self.gamma * q_next.max(1)[0].view(self.batch_size, 1) # 計算這些 experience 當下 target net 所得出的 Q value\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 每隔一段時間 (target_replace_iter), 更新 target net，即複製 eval net 到 target net\n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.target_replace_iter == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            \n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    return history.obs_list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #plt.plot([exchange.profit])\n",
    "    #plt.show()\n",
    "    return exchange.profit\n",
    "\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "# Environment parameters\n",
    "n_actions = 3\n",
    "n_states = 51\n",
    "\n",
    "# Hyper parameters\n",
    "n_hidden = 10\n",
    "batch_size = 32\n",
    "lr = 0.01                 # learning rate\n",
    "epsilon = 0.1             # epsilon-greedy\n",
    "gamma = 0.9               # reward discount factor\n",
    "target_replace_iter = 100 # target network 更新間隔\n",
    "memory_capacity = 5000\n",
    "n_episodes = 1\n",
    "\n",
    "\n",
    "# 建立 DQN\n",
    "dqn = DQN(n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity)\n",
    "\n",
    "# 學習\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "\n",
    "        # 選擇 action\n",
    "        action = dqn.choose_action(obs0)\n",
    "        #obs, reward, done, info = env.step(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # 儲存 experience\n",
    "        dqn.store_transition(obs0, action, reward, obs)\n",
    "\n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > memory_capacity:\n",
    "            dqn.learn()\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = obs\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n",
    "            \n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.1\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 1 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            #print(\"rd\")\n",
    "            #print(action)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"total\"] > 50):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if done:\n",
    "            torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 1\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 5000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = self._build_model(), self._build_model()\n",
    "\n",
    "        self.learn_step_counter = 0        # for target updating\n",
    "        self.memory_counter = 0            # for storing memory\n",
    "        self.memory = deque(maxlen=2000)   # initialize memory\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=N_STATES, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(N_ACTIONS, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=LR))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = np.array(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.predict(x)\n",
    "            action = np.argmax(actions_value[0])   \n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        self.memory.append((s, a, r, s_))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"total\"] > 0):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if done:\n",
    "            torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
