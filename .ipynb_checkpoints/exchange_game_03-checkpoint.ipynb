{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mpl_finance.py:22: DeprecationWarning: \n",
      "\n",
      "  =================================================================\n",
      "\n",
      "   WARNING: `mpl_finance` is deprecated:\n",
      "\n",
      "    Please use `mplfinance` instead (no hyphen, no underscore).\n",
      "\n",
      "    To install: `pip install --upgrade mplfinance` \n",
      "\n",
      "   For more information, see: https://pypi.org/project/mplfinance/\n",
      "\n",
      "  =================================================================\n",
      "\n",
      "  category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "episode: 1/5000, score: 80, e: 0.48\n",
      "total profit: -150.08716149915114\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 5000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "agent = DQNAgent(N_STATES, N_ACTIONS)\n",
    "# agent.load(PATH)\n",
    "done = False\n",
    "\n",
    "\n",
    "for e in range(n_episodes):\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, N_STATES])\n",
    "    for time in range(100):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, N_STATES])\n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, n_episodes, time, agent.epsilon))\n",
    "            print('total profit: {}'\n",
    "                  .format(info[\"profit\"][\"total\"]))\n",
    "            break\n",
    "        if len(agent.memory) > BATCH_SIZE:\n",
    "            agent.replay(BATCH_SIZE)\n",
    "        \n",
    "\n",
    "            \n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 1 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            #print(\"rd\")\n",
    "            #print(action)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"total\"] > 50):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if done:\n",
    "            torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
