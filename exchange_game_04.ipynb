{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "(1) finished after 1550 timesteps, total profit: -378.10766847989504, memery: 0\n",
      "(2) finished after 1191 timesteps, total profit: -354.85142172681975, memery: 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "#matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    if(exchange.floating_profit > 0):\n",
    "        reward = exchange.floating_profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    else:\n",
    "        reward = exchange.profit * 0.001\n",
    "    \n",
    "    \n",
    "    reward = exchange.floating_profit * (exchange.info[\"index\"] - 50) * 0.01\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    #print(reward)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 1000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.fc2 = nn.Linear(50, 50).to(device)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, 50).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"floating\"] > 20):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if(done):\n",
    "            #torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
