{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    return history.obs_list\n",
    "\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "done = False\n",
    "rewards = 0\n",
    "obs = env.reset()\n",
    "for i in range(101):\n",
    "    action = random.sample([0, 1, 2], 1)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    #env.render()\n",
    "    #rewards += reward\n",
    "\n",
    "    if(i%100 == 0):\n",
    "        print('{}K ({})'.format(i+1,reward))\n",
    "\n",
    "        print(obs.shape[0])\n",
    "      \n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # 輸入層 (state) 到隱藏層，隱藏層到輸出層 (action)\n",
    "        self.fc1 = nn.Linear(n_states, n_hidden)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(n_hidden, n_actions)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x) # ReLU activation\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value\n",
    "    \n",
    "class DQN(object):\n",
    "    def __init__(self, n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity):\n",
    "        self.eval_net, self.target_net = Net(n_states, n_actions, n_hidden), Net(n_states, n_actions, n_hidden)\n",
    "\n",
    "        self.memory = np.zeros((memory_capacity, n_states * 2 + 2)) # 每個 memory 中的 experience 大小為 (state + next state + reward + action)\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=lr)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "        self.memory_counter = 0\n",
    "        self.learn_step_counter = 0 # 讓 target network 知道什麼時候要更新\n",
    "\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.n_hidden = n_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.target_replace_iter = target_replace_iter\n",
    "        self.memory_capacity = memory_capacity\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(state), 0)\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if np.random.uniform() < self.epsilon: # 隨機\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        else: # 根據現有 policy 做最好的選擇\n",
    "            actions_value = self.eval_net.forward(x) # 以現有 eval net 得出各個 action 的分數\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()[0] # 挑選最高分的 action\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # 打包 experience\n",
    "        transition = np.hstack((state, [action, reward], next_state))\n",
    "\n",
    "        # 存進 memory；舊 memory 可能會被覆蓋\n",
    "        index = self.memory_counter % self.memory_capacity\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # 隨機取樣 batch_size 個 experience\n",
    "        sample_index = np.random.choice(self.memory_capacity, self.batch_size)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_state = torch.FloatTensor(b_memory[:, :self.n_states])\n",
    "        b_action = torch.LongTensor(b_memory[:, self.n_states:self.n_states+1].astype(int))\n",
    "        b_reward = torch.FloatTensor(b_memory[:, self.n_states+1:self.n_states+2])\n",
    "        b_next_state = torch.FloatTensor(b_memory[:, -self.n_states:])\n",
    "\n",
    "        # 計算現有 eval net 和 target net 得出 Q value 的落差\n",
    "        q_eval = self.eval_net(b_state).gather(1, b_action) # 重新計算這些 experience 當下 eval net 所得出的 Q value\n",
    "        q_next = self.target_net(b_next_state).detach() # detach 才不會訓練到 target net\n",
    "        q_target = b_reward + self.gamma * q_next.max(1)[0].view(self.batch_size, 1) # 計算這些 experience 當下 target net 所得出的 Q value\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 每隔一段時間 (target_replace_iter), 更新 target net，即複製 eval net 到 target net\n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.target_replace_iter == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            \n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    return history.obs_list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #plt.plot([exchange.profit])\n",
    "    #plt.show()\n",
    "    return exchange.profit\n",
    "\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000, data_kwargs={'use_ta': False})\n",
    "\n",
    "# Environment parameters\n",
    "n_actions = 3\n",
    "n_states = 51\n",
    "\n",
    "# Hyper parameters\n",
    "n_hidden = 10\n",
    "batch_size = 32\n",
    "lr = 0.01                 # learning rate\n",
    "epsilon = 0.1             # epsilon-greedy\n",
    "gamma = 0.9               # reward discount factor\n",
    "target_replace_iter = 100 # target network 更新間隔\n",
    "memory_capacity = 5000\n",
    "n_episodes = 1\n",
    "\n",
    "\n",
    "# 建立 DQN\n",
    "dqn = DQN(n_states, n_actions, n_hidden, batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity)\n",
    "\n",
    "# 學習\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "\n",
    "        # 選擇 action\n",
    "        action = dqn.choose_action(obs0)\n",
    "        #obs, reward, done, info = env.step(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        # 儲存 experience\n",
    "        dqn.store_transition(obs0, action, reward, obs)\n",
    "\n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > memory_capacity:\n",
    "            dqn.learn()\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = obs\n",
    "\n",
    "        if done:\n",
    "            print('Episode finished after {} timesteps, total rewards {}'.format(t+1, rewards))\n",
    "            \n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mpl_finance.py:22: DeprecationWarning: \n",
      "\n",
      "  =================================================================\n",
      "\n",
      "   WARNING: `mpl_finance` is deprecated:\n",
      "\n",
      "    Please use `mplfinance` instead (no hyphen, no underscore).\n",
      "\n",
      "    To install: `pip install --upgrade mplfinance` \n",
      "\n",
      "   For more information, see: https://pypi.org/project/mplfinance/\n",
      "\n",
      "  =================================================================\n",
      "\n",
      "  category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.1\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 1 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, N_ACTIONS).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            #print(\"rd\")\n",
    "            #print(action)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"total\"] > 50):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if done:\n",
    "            torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.profit * (exchange.info[\"index\"] - 50) * 0.001\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 1\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 5000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = self._build_model(), self._build_model()\n",
    "\n",
    "        self.learn_step_counter = 0        # for target updating\n",
    "        self.memory_counter = 0            # for storing memory\n",
    "        self.memory = deque(maxlen=2000)   # initialize memory\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=N_STATES, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(N_ACTIONS, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=LR))\n",
    "        return model\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        x = np.array(x)\n",
    "        #print(x.shape)\n",
    "        #print(x)\n",
    "        #x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.predict(x)\n",
    "            action = np.argmax(actions_value[0])   \n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        self.memory.append((s, a, r, s_))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"total\"] > 0):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if done:\n",
    "            torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mpl_finance.py:22: DeprecationWarning: \n",
      "\n",
      "  =================================================================\n",
      "\n",
      "   WARNING: `mpl_finance` is deprecated:\n",
      "\n",
      "    Please use `mplfinance` instead (no hyphen, no underscore).\n",
      "\n",
      "    To install: `pip install --upgrade mplfinance` \n",
      "\n",
      "   For more information, see: https://pypi.org/project/mplfinance/\n",
      "\n",
      "  =================================================================\n",
      "\n",
      "  category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting experience...\n",
      "(1) finished after 458 timesteps, total profit: -150.10545705795613, memery: 0\n",
      "(2) finished after 16641 timesteps, total profit: -156.94728145367512, memery: 3773\n",
      "(3) finished after 1015 timesteps, total profit: -157.27933216977442, memery: 3773\n",
      "(4) finished after 583 timesteps, total profit: -151.21431604074905, memery: 3773\n",
      "(5) finished after 1055 timesteps, total profit: -155.64108929256173, memery: 3773\n",
      "(6) finished after 1382 timesteps, total profit: -161.89262965308728, memery: 3773\n",
      "(7) finished after 1249 timesteps, total profit: -156.37461766290107, memery: 3773\n",
      "(8) finished after 254 timesteps, total profit: -168.8933014681164, memery: 3773\n",
      "(9) finished after 1705 timesteps, total profit: -150.0297673451274, memery: 3773\n",
      "(10) finished after 7158 timesteps, total profit: -169.8803642470407, memery: 4395\n",
      "(11) finished after 1411 timesteps, total profit: -161.56135271415957, memery: 4395\n",
      "(12) finished after 1196 timesteps, total profit: -170.9514422589982, memery: 4395\n",
      "(13) finished after 442 timesteps, total profit: -159.41695872592214, memery: 4395\n",
      "(14) finished after 1117 timesteps, total profit: -155.242194373606, memery: 4395\n",
      "(15) finished after 1509 timesteps, total profit: -158.98735415560347, memery: 4395\n",
      "(16) finished after 395 timesteps, total profit: -167.80320477605085, memery: 4395\n",
      "(17) finished after 1506 timesteps, total profit: -150.04527440471, memery: 4395\n",
      "(18) finished after 1327 timesteps, total profit: -151.90904805587854, memery: 4395\n",
      "(19) finished after 111 timesteps, total profit: -163.68743048912594, memery: 4395\n",
      "(20) finished after 1426 timesteps, total profit: -162.35362630177403, memery: 4395\n",
      "(21) finished after 163 timesteps, total profit: -160.87483561304515, memery: 4395\n",
      "(22) finished after 1618 timesteps, total profit: -156.90284175572856, memery: 4395\n",
      "(23) finished after 1435 timesteps, total profit: -160.8222176598398, memery: 4395\n",
      "(24) finished after 392 timesteps, total profit: -163.56947430864705, memery: 4395\n",
      "(25) finished after 1239 timesteps, total profit: -172.63959743137627, memery: 4395\n",
      "(26) finished after 370 timesteps, total profit: -159.82203261142365, memery: 4395\n",
      "(27) finished after 367 timesteps, total profit: -165.32618530128315, memery: 4395\n",
      "(28) finished after 692 timesteps, total profit: -156.4208748388, memery: 4395\n",
      "(29) finished after 1704 timesteps, total profit: -150.3732979855346, memery: 4395\n",
      "(30) finished after 601 timesteps, total profit: -157.7783558852246, memery: 4395\n",
      "(31) finished after 311 timesteps, total profit: -159.25282546537127, memery: 4395\n",
      "(32) finished after 460 timesteps, total profit: -167.711650114254, memery: 4395\n",
      "(33) finished after 543 timesteps, total profit: -166.41348252666754, memery: 4395\n",
      "(34) finished after 754 timesteps, total profit: -171.5909925612745, memery: 4395\n",
      "(35) finished after 679 timesteps, total profit: -172.86247000119474, memery: 4395\n",
      "(36) finished after 342 timesteps, total profit: -176.67582043332973, memery: 4395\n",
      "(37) finished after 473 timesteps, total profit: -172.9824898774172, memery: 4395\n",
      "(38) finished after 619 timesteps, total profit: -165.4834413986035, memery: 4395\n",
      "(39) finished after 412 timesteps, total profit: -171.43536914869503, memery: 4395\n",
      "(40) finished after 194 timesteps, total profit: -166.8824781982698, memery: 4395\n",
      "(41) finished after 426 timesteps, total profit: -169.73508380683816, memery: 4395\n",
      "(42) finished after 1372 timesteps, total profit: -150.76484950660722, memery: 4395\n",
      "(43) finished after 688 timesteps, total profit: -166.91255076166595, memery: 4395\n",
      "(44) finished after 391 timesteps, total profit: -157.9062950921648, memery: 4395\n",
      "(45) finished after 1496 timesteps, total profit: -150.01924815752992, memery: 4395\n",
      "(46) finished after 498 timesteps, total profit: -152.9246414494262, memery: 4395\n",
      "(47) finished after 1050 timesteps, total profit: -176.56754286908844, memery: 4395\n",
      "(48) finished after 606 timesteps, total profit: -164.72699805291842, memery: 4395\n",
      "(49) finished after 1406 timesteps, total profit: -161.9492246544225, memery: 4395\n",
      "(50) finished after 240 timesteps, total profit: -175.36727067887853, memery: 4395\n",
      "(51) finished after 1391 timesteps, total profit: -150.36758345042583, memery: 4395\n",
      "(52) finished after 201 timesteps, total profit: -168.8669299807121, memery: 4395\n",
      "(53) finished after 132 timesteps, total profit: -166.2605565126409, memery: 4395\n",
      "(54) finished after 746 timesteps, total profit: -157.99611406746138, memery: 4395\n",
      "(55) finished after 1349 timesteps, total profit: -158.32134894608086, memery: 4395\n",
      "(56) finished after 220 timesteps, total profit: -166.8137609962015, memery: 4395\n",
      "(57) finished after 26225 timesteps, total profit: -207.0359140049404, memery: 7008\n",
      "(58) finished after 1237 timesteps, total profit: -152.0837303129738, memery: 7009\n",
      "(59) finished after 1390 timesteps, total profit: -164.84720594166745, memery: 7010\n",
      "(60) finished after 798 timesteps, total profit: -156.00806603256785, memery: 7010\n",
      "(61) finished after 197 timesteps, total profit: -166.35058408747707, memery: 7010\n",
      "(62) finished after 423 timesteps, total profit: -153.30779756849665, memery: 7010\n",
      "(63) finished after 1328 timesteps, total profit: -155.17169635535225, memery: 7013\n",
      "(64) finished after 1324 timesteps, total profit: -162.67595204323106, memery: 7014\n",
      "(65) finished after 283 timesteps, total profit: -162.78126737315336, memery: 7014\n",
      "(66) finished after 1360 timesteps, total profit: -169.7588403763882, memery: 7015\n",
      "(67) finished after 1408 timesteps, total profit: -150.43301352498804, memery: 7017\n",
      "(68) finished after 283 timesteps, total profit: -172.23707954233078, memery: 7017\n",
      "(69) finished after 1378 timesteps, total profit: -169.5038618401207, memery: 7017\n",
      "(70) finished after 1318 timesteps, total profit: -173.83194733742613, memery: 7018\n",
      "(71) finished after 1509 timesteps, total profit: -153.39012505388598, memery: 7051\n",
      "(72) finished after 432 timesteps, total profit: -168.8435244554745, memery: 7051\n",
      "(73) finished after 1426 timesteps, total profit: -152.3386753731436, memery: 7053\n",
      "(74) finished after 548 timesteps, total profit: -159.382596879452, memery: 7053\n",
      "(75) finished after 1204 timesteps, total profit: -228.1940069925717, memery: 7056\n",
      "(76) finished after 1390 timesteps, total profit: -155.86019297955562, memery: 7056\n",
      "(77) finished after 360 timesteps, total profit: -164.24426869653092, memery: 7056\n",
      "(78) finished after 1196 timesteps, total profit: -153.42583457626895, memery: 7056\n",
      "(79) finished after 1314 timesteps, total profit: -167.20485231763396, memery: 7056\n",
      "(80) finished after 449 timesteps, total profit: -152.7720172442074, memery: 7056\n",
      "(81) finished after 1462 timesteps, total profit: -159.16495741812514, memery: 7056\n",
      "(82) finished after 1386 timesteps, total profit: -160.96393856237023, memery: 7060\n",
      "(83) finished after 1174 timesteps, total profit: -154.49964837215745, memery: 7060\n",
      "(84) finished after 1368 timesteps, total profit: -155.78856170798306, memery: 7062\n",
      "(85) finished after 281 timesteps, total profit: -151.5133479485652, memery: 7062\n",
      "(86) finished after 1540 timesteps, total profit: -151.28837843311896, memery: 7063\n",
      "(87) finished after 177 timesteps, total profit: -162.31955174603462, memery: 7063\n",
      "(88) finished after 1259 timesteps, total profit: -165.3639929790831, memery: 7063\n",
      "(89) finished after 1346 timesteps, total profit: -151.71162111296277, memery: 7064\n",
      "(90) finished after 176 timesteps, total profit: -168.63431946934196, memery: 7064\n",
      "(91) finished after 1574 timesteps, total profit: -169.32845057385856, memery: 7093\n",
      "(92) finished after 450 timesteps, total profit: -155.90158485326822, memery: 7093\n",
      "(93) finished after 650 timesteps, total profit: -158.1826822327056, memery: 7093\n",
      "(94) finished after 1394 timesteps, total profit: -177.95883858116815, memery: 7095\n",
      "(95) finished after 1197 timesteps, total profit: -203.0128625119741, memery: 7097\n",
      "(96) finished after 217 timesteps, total profit: -164.20811368419245, memery: 7097\n",
      "(97) finished after 1647 timesteps, total profit: -154.4020470807023, memery: 7097\n",
      "(98) finished after 1285 timesteps, total profit: -163.9610846861334, memery: 7098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99) finished after 215 timesteps, total profit: -164.55365850365666, memery: 7098\n",
      "(100) finished after 543 timesteps, total profit: -150.37238971054308, memery: 7098\n",
      "(101) finished after 1209 timesteps, total profit: -163.26247435107933, memery: 7099\n",
      "(102) finished after 876 timesteps, total profit: -159.45972621357075, memery: 7100\n",
      "(103) finished after 1251 timesteps, total profit: -152.81684077155037, memery: 7100\n",
      "(104) finished after 1372 timesteps, total profit: -162.61912038926076, memery: 7100\n",
      "(105) finished after 1587 timesteps, total profit: -171.39770853008062, memery: 7106\n",
      "(106) finished after 1701 timesteps, total profit: -154.62327301372025, memery: 7128\n",
      "(107) finished after 172 timesteps, total profit: -168.1152473126777, memery: 7128\n",
      "(108) finished after 1647 timesteps, total profit: -152.96155015529718, memery: 7181\n",
      "(109) finished after 1193 timesteps, total profit: -157.02596162244217, memery: 7181\n",
      "(110) finished after 1322 timesteps, total profit: -180.13947771140514, memery: 7182\n",
      "(111) finished after 1471 timesteps, total profit: -162.91145121393825, memery: 7192\n",
      "(112) finished after 1280 timesteps, total profit: -184.16535352881772, memery: 7193\n",
      "(113) finished after 1270 timesteps, total profit: -176.04081584899546, memery: 7221\n",
      "(114) finished after 1287 timesteps, total profit: -163.47020800632626, memery: 7222\n",
      "(115) finished after 945 timesteps, total profit: -150.74394871033033, memery: 7222\n",
      "(116) finished after 263 timesteps, total profit: -158.3025869166749, memery: 7222\n",
      "(117) finished after 176 timesteps, total profit: -163.5263814172074, memery: 7222\n",
      "(118) finished after 221 timesteps, total profit: -150.60902013674314, memery: 7222\n",
      "(119) finished after 1242 timesteps, total profit: -177.606888075382, memery: 7225\n",
      "(120) finished after 1200 timesteps, total profit: -153.48025434658945, memery: 7225\n",
      "(121) finished after 711 timesteps, total profit: -158.80458230638934, memery: 7251\n",
      "(122) finished after 1427 timesteps, total profit: -187.09252971569992, memery: 7253\n",
      "(123) finished after 1195 timesteps, total profit: -168.34394295539767, memery: 7254\n",
      "(124) finished after 1191 timesteps, total profit: -157.20515620988186, memery: 7254\n",
      "(125) finished after 304 timesteps, total profit: -164.1085977570189, memery: 7254\n",
      "(126) finished after 838 timesteps, total profit: -157.8326583462817, memery: 7254\n",
      "(127) finished after 1195 timesteps, total profit: -153.01253830289787, memery: 7256\n",
      "(128) finished after 706 timesteps, total profit: -163.11572557591555, memery: 7257\n",
      "(129) finished after 1370 timesteps, total profit: -152.58283174812095, memery: 7258\n",
      "(130) finished after 413 timesteps, total profit: -150.06609120170722, memery: 7258\n",
      "(131) finished after 4802 timesteps, total profit: -153.54125497448413, memery: 7432\n",
      "(132) finished after 995 timesteps, total profit: -158.55631159854025, memery: 7433\n",
      "(133) finished after 72 timesteps, total profit: -150.02520867660877, memery: 7433\n",
      "(134) finished after 1444 timesteps, total profit: -163.07060668908997, memery: 7436\n",
      "(135) finished after 1198 timesteps, total profit: -160.82303498754908, memery: 7439\n",
      "(136) finished after 886 timesteps, total profit: -152.01514551087698, memery: 7440\n",
      "(137) finished after 701 timesteps, total profit: -160.68067628788938, memery: 7440\n",
      "(138) finished after 1291 timesteps, total profit: -162.54839220154332, memery: 7440\n",
      "(139) finished after 596 timesteps, total profit: -151.47230748136516, memery: 7441\n",
      "(140) finished after 212 timesteps, total profit: -156.61180916517623, memery: 7441\n",
      "(141) finished after 1202 timesteps, total profit: -183.67236481959628, memery: 7442\n",
      "(142) finished after 276 timesteps, total profit: -165.18892476190933, memery: 7442\n",
      "(143) finished after 1336 timesteps, total profit: -150.18494396725004, memery: 7443\n",
      "(144) finished after 438 timesteps, total profit: -171.0151143427743, memery: 7443\n",
      "(145) finished after 632 timesteps, total profit: -150.72554320787228, memery: 7443\n",
      "(146) finished after 550 timesteps, total profit: -155.51257321067, memery: 7444\n",
      "(147) finished after 1328 timesteps, total profit: -154.829915297285, memery: 7446\n",
      "(148) finished after 928 timesteps, total profit: -156.38586322860874, memery: 7447\n",
      "(149) finished after 655 timesteps, total profit: -152.89631285134018, memery: 7447\n",
      "(150) finished after 424 timesteps, total profit: -170.98530208530187, memery: 7447\n",
      "(151) finished after 283 timesteps, total profit: -154.72407297782397, memery: 7447\n",
      "(152) finished after 688 timesteps, total profit: -152.14153006604246, memery: 7447\n",
      "(153) finished after 960 timesteps, total profit: -151.61706844801847, memery: 7448\n",
      "(154) finished after 360 timesteps, total profit: -167.28421730578285, memery: 7448\n",
      "(155) finished after 1620 timesteps, total profit: -181.02166603384066, memery: 7458\n",
      "(156) finished after 947 timesteps, total profit: -153.9402993160304, memery: 7459\n",
      "(157) finished after 143 timesteps, total profit: -150.85409514128173, memery: 7459\n",
      "(158) finished after 1214 timesteps, total profit: -291.3446962213734, memery: 7474\n",
      "(159) finished after 183 timesteps, total profit: -160.5839026542977, memery: 7474\n",
      "(160) finished after 1324 timesteps, total profit: -152.57839045335538, memery: 7475\n",
      "(161) finished after 207 timesteps, total profit: -166.59881757117228, memery: 7475\n",
      "(162) finished after 1280 timesteps, total profit: -151.4968215688949, memery: 7475\n",
      "(163) finished after 309 timesteps, total profit: -174.62176674794637, memery: 7475\n",
      "(164) finished after 211 timesteps, total profit: -164.92107359895982, memery: 7475\n",
      "(165) finished after 498 timesteps, total profit: -150.2833949550774, memery: 7475\n",
      "(166) finished after 222 timesteps, total profit: -162.91187372616773, memery: 7475\n",
      "(167) finished after 1407 timesteps, total profit: -181.40387357372794, memery: 7478\n",
      "(168) finished after 1344 timesteps, total profit: -164.07207862749723, memery: 7480\n",
      "(169) finished after 211 timesteps, total profit: -152.8791320016881, memery: 7480\n",
      "(170) finished after 1117 timesteps, total profit: -173.8896839495249, memery: 7481\n",
      "(171) finished after 707 timesteps, total profit: -185.8205678469131, memery: 7482\n",
      "(172) finished after 761 timesteps, total profit: -150.33367493235622, memery: 7482\n",
      "(173) finished after 705 timesteps, total profit: -161.7058944975969, memery: 7482\n",
      "(174) finished after 1332 timesteps, total profit: -164.86066089118367, memery: 7483\n",
      "(175) finished after 1196 timesteps, total profit: -158.43684056218592, memery: 7485\n",
      "(176) finished after 80 timesteps, total profit: -156.97295663641677, memery: 7485\n",
      "(177) finished after 277 timesteps, total profit: -150.72568950265992, memery: 7485\n",
      "(178) finished after 699 timesteps, total profit: -152.57664695363576, memery: 7485\n",
      "(179) finished after 953 timesteps, total profit: -150.7967304675663, memery: 7485\n",
      "(180) finished after 595 timesteps, total profit: -165.5739933922489, memery: 7485\n",
      "(181) finished after 543 timesteps, total profit: -153.50075678586896, memery: 7485\n",
      "(182) finished after 1502 timesteps, total profit: -162.84934610185098, memery: 7509\n",
      "(183) finished after 1572 timesteps, total profit: -159.35526770593012, memery: 7520\n",
      "(184) finished after 684 timesteps, total profit: -157.01589505413088, memery: 7522\n",
      "(185) finished after 1368 timesteps, total profit: -168.7484434392351, memery: 7527\n",
      "(186) finished after 268 timesteps, total profit: -164.43143170671019, memery: 7527\n",
      "(187) finished after 1395 timesteps, total profit: -164.6831903090806, memery: 7529\n",
      "(188) finished after 1331 timesteps, total profit: -170.79542045069235, memery: 7533\n",
      "(189) finished after 451 timesteps, total profit: -162.16479682094294, memery: 7533\n",
      "(190) finished after 1320 timesteps, total profit: -152.09823609990613, memery: 7535\n",
      "(191) finished after 259 timesteps, total profit: -155.85429844380647, memery: 7535\n",
      "(192) finished after 662 timesteps, total profit: -159.44601671199, memery: 7535\n",
      "(193) finished after 267 timesteps, total profit: -150.2277518212589, memery: 7535\n",
      "(194) finished after 271 timesteps, total profit: -151.28571606623723, memery: 7535\n",
      "(195) finished after 968 timesteps, total profit: -166.00868220653967, memery: 7537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196) finished after 1236 timesteps, total profit: -156.1387306207529, memery: 7538\n",
      "(197) finished after 179 timesteps, total profit: -163.3885684279933, memery: 7538\n",
      "(198) finished after 275 timesteps, total profit: -163.4958220338202, memery: 7538\n",
      "(199) finished after 1448 timesteps, total profit: -173.82239634581808, memery: 7539\n",
      "(200) finished after 248 timesteps, total profit: -170.81611206895104, memery: 7539\n",
      "(201) finished after 701 timesteps, total profit: -154.94592523620693, memery: 7539\n",
      "(202) finished after 1391 timesteps, total profit: -160.23066564156957, memery: 7543\n",
      "(203) finished after 205 timesteps, total profit: -150.8765885248733, memery: 7543\n",
      "(204) finished after 1261 timesteps, total profit: -160.1903047038312, memery: 7543\n",
      "(205) finished after 302 timesteps, total profit: -157.6053073019017, memery: 7543\n",
      "(206) finished after 674 timesteps, total profit: -151.46456380856668, memery: 7543\n",
      "(207) finished after 730 timesteps, total profit: -170.22717588833325, memery: 7543\n",
      "(208) finished after 399 timesteps, total profit: -150.06780013984323, memery: 7543\n",
      "(209) finished after 289 timesteps, total profit: -150.6344277804954, memery: 7543\n",
      "(210) finished after 683 timesteps, total profit: -155.65080847848282, memery: 7543\n",
      "(211) finished after 582 timesteps, total profit: -151.7591174101192, memery: 7543\n",
      "(212) finished after 1348 timesteps, total profit: -156.7696154689296, memery: 7545\n",
      "(213) finished after 1311 timesteps, total profit: -190.62850224760695, memery: 7545\n",
      "(214) finished after 798 timesteps, total profit: -150.00467825280072, memery: 7547\n",
      "(215) finished after 246 timesteps, total profit: -170.33066703904018, memery: 7547\n",
      "(216) finished after 1232 timesteps, total profit: -152.55168889928754, memery: 7549\n",
      "(217) finished after 1205 timesteps, total profit: -196.39807512404, memery: 7552\n",
      "(218) finished after 219 timesteps, total profit: -158.44821643595213, memery: 7552\n",
      "(219) finished after 934 timesteps, total profit: -173.34847187182928, memery: 7552\n",
      "(220) finished after 1202 timesteps, total profit: -161.28556108926026, memery: 7552\n",
      "(221) finished after 708 timesteps, total profit: -150.96541064753404, memery: 7552\n",
      "(222) finished after 1214 timesteps, total profit: -154.73578969834165, memery: 7554\n",
      "(223) finished after 1180 timesteps, total profit: -162.34557559206348, memery: 7556\n",
      "(224) finished after 770 timesteps, total profit: -165.02639622316514, memery: 7557\n",
      "(225) finished after 1428 timesteps, total profit: -152.9405279704578, memery: 7559\n",
      "(226) finished after 482 timesteps, total profit: -167.1420660457341, memery: 7559\n",
      "(227) finished after 1211 timesteps, total profit: -155.24655584863893, memery: 7560\n",
      "(228) finished after 1206 timesteps, total profit: -167.65684363716562, memery: 7560\n",
      "(229) finished after 419 timesteps, total profit: -152.57734890663122, memery: 7560\n",
      "(230) finished after 713 timesteps, total profit: -151.63986228109198, memery: 7560\n",
      "(231) finished after 978 timesteps, total profit: -156.07360402413906, memery: 7561\n",
      "(232) finished after 880 timesteps, total profit: -157.0721499697813, memery: 7561\n",
      "(233) finished after 656 timesteps, total profit: -173.71319483239222, memery: 7561\n",
      "(234) finished after 781 timesteps, total profit: -173.4917362757886, memery: 7562\n",
      "(235) finished after 1224 timesteps, total profit: -171.30154548510808, memery: 7563\n",
      "(236) finished after 1246 timesteps, total profit: -151.55430326248043, memery: 7563\n",
      "(237) finished after 1390 timesteps, total profit: -155.1856577320621, memery: 7566\n",
      "(238) finished after 1351 timesteps, total profit: -163.5076912751954, memery: 7568\n",
      "(239) finished after 1360 timesteps, total profit: -153.6669238818259, memery: 7569\n",
      "(240) finished after 1397 timesteps, total profit: -163.09519017012644, memery: 7571\n",
      "(241) finished after 1389 timesteps, total profit: -169.79320899269143, memery: 7579\n",
      "(242) finished after 429 timesteps, total profit: -150.96716483671528, memery: 7579\n",
      "(243) finished after 1314 timesteps, total profit: -166.85172648526068, memery: 7579\n",
      "(244) finished after 1522 timesteps, total profit: -163.6757527298745, memery: 7616\n",
      "(245) finished after 1275 timesteps, total profit: -155.1605738759747, memery: 7617\n",
      "(246) finished after 1255 timesteps, total profit: -152.4707159260471, memery: 7618\n",
      "(247) finished after 1460 timesteps, total profit: -185.3625995205925, memery: 7621\n",
      "(248) finished after 1206 timesteps, total profit: -162.0719047347176, memery: 7621\n",
      "(249) finished after 684 timesteps, total profit: -151.0756157476749, memery: 7621\n",
      "(250) finished after 1019 timesteps, total profit: -172.45268107444213, memery: 7621\n",
      "(251) finished after 1376 timesteps, total profit: -155.30113282854967, memery: 7622\n",
      "(252) finished after 1199 timesteps, total profit: -157.43468731846517, memery: 7622\n",
      "(253) finished after 1087 timesteps, total profit: -167.65839704234145, memery: 7643\n",
      "(254) finished after 238 timesteps, total profit: -171.0642382567554, memery: 7643\n",
      "(255) finished after 273 timesteps, total profit: -156.08735218309042, memery: 7643\n",
      "(256) finished after 686 timesteps, total profit: -166.1838030249923, memery: 7643\n",
      "(257) finished after 1354 timesteps, total profit: -156.6579907090863, memery: 7645\n",
      "(258) finished after 1001 timesteps, total profit: -154.7546846358721, memery: 7646\n",
      "(259) finished after 517 timesteps, total profit: -166.65099752936516, memery: 7646\n",
      "(260) finished after 1512 timesteps, total profit: -154.60377149568845, memery: 7649\n",
      "(261) finished after 1327 timesteps, total profit: -157.69140571642492, memery: 7653\n",
      "(262) finished after 1198 timesteps, total profit: -215.98457469701935, memery: 7655\n",
      "(263) finished after 244 timesteps, total profit: -153.85488367261675, memery: 7655\n",
      "(264) finished after 1388 timesteps, total profit: -170.2292899619423, memery: 7656\n",
      "(265) finished after 1200 timesteps, total profit: -161.39845948137614, memery: 7657\n",
      "(266) finished after 1335 timesteps, total profit: -150.14877743572876, memery: 7658\n",
      "(267) finished after 1308 timesteps, total profit: -158.4405063639123, memery: 7659\n",
      "(268) finished after 1493 timesteps, total profit: -167.29881249723346, memery: 7672\n",
      "(269) finished after 512 timesteps, total profit: -161.42303364405325, memery: 7672\n",
      "(270) finished after 1468 timesteps, total profit: -160.25413476445243, memery: 7700\n",
      "(271) finished after 182 timesteps, total profit: -157.79053754200268, memery: 7700\n",
      "(272) finished after 1218 timesteps, total profit: -160.44036910791533, memery: 7700\n",
      "(273) finished after 577 timesteps, total profit: -159.73385981617906, memery: 7703\n",
      "(274) finished after 1319 timesteps, total profit: -157.48716638349526, memery: 7705\n",
      "(275) finished after 739 timesteps, total profit: -151.15559060603726, memery: 7705\n",
      "(276) finished after 304 timesteps, total profit: -175.29009393599318, memery: 7705\n",
      "(277) finished after 526 timesteps, total profit: -167.5439084786243, memery: 7705\n",
      "(278) finished after 1390 timesteps, total profit: -156.29275322517134, memery: 7707\n",
      "(279) finished after 218 timesteps, total profit: -170.10100985330564, memery: 7707\n",
      "(280) finished after 700 timesteps, total profit: -153.8773464639816, memery: 7707\n",
      "(281) finished after 1391 timesteps, total profit: -161.2121463177132, memery: 7707\n",
      "(282) finished after 435 timesteps, total profit: -175.74174568853698, memery: 7707\n",
      "(283) finished after 1406 timesteps, total profit: -199.96358461782094, memery: 7707\n",
      "(284) finished after 710 timesteps, total profit: -153.02569499098263, memery: 7707\n",
      "(285) finished after 622 timesteps, total profit: -165.30480635130434, memery: 7707\n",
      "(286) finished after 1361 timesteps, total profit: -154.74432127488615, memery: 7708\n",
      "(287) finished after 1256 timesteps, total profit: -152.96743440082366, memery: 7709\n",
      "(288) finished after 1203 timesteps, total profit: -150.61974061003565, memery: 7709\n",
      "(289) finished after 1399 timesteps, total profit: -159.91288282413205, memery: 7711\n",
      "(290) finished after 182 timesteps, total profit: -176.27704704033795, memery: 7711\n",
      "(291) finished after 1191 timesteps, total profit: -155.59794572843413, memery: 7712\n",
      "(292) finished after 1194 timesteps, total profit: -168.22626724388834, memery: 7712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293) finished after 415 timesteps, total profit: -150.37941811416042, memery: 7712\n",
      "(294) finished after 1379 timesteps, total profit: -157.56365369967284, memery: 7712\n",
      "(295) finished after 1004 timesteps, total profit: -150.1606040328922, memery: 7714\n",
      "(296) finished after 1285 timesteps, total profit: -155.54873413078775, memery: 7714\n",
      "(297) finished after 222 timesteps, total profit: -160.9014965994242, memery: 7714\n",
      "(298) finished after 620 timesteps, total profit: -153.80096116981065, memery: 7715\n",
      "(299) finished after 1255 timesteps, total profit: -196.54421311803077, memery: 7716\n",
      "(300) finished after 1763 timesteps, total profit: -150.47614391926902, memery: 7717\n",
      "(301) finished after 1383 timesteps, total profit: -150.33741077980136, memery: 7720\n",
      "(302) finished after 916 timesteps, total profit: -153.2721903332284, memery: 7720\n",
      "(303) finished after 4200 timesteps, total profit: -171.94742436352536, memery: 7801\n",
      "(304) finished after 222 timesteps, total profit: -166.10384594914123, memery: 7801\n",
      "(305) finished after 434 timesteps, total profit: -163.83450401686343, memery: 7801\n",
      "(306) finished after 181 timesteps, total profit: -154.16545804110555, memery: 7801\n",
      "(307) finished after 417 timesteps, total profit: -150.84205277762757, memery: 7801\n",
      "(308) finished after 1232 timesteps, total profit: -152.8900516873786, memery: 7803\n",
      "(309) finished after 1206 timesteps, total profit: -163.76465122848452, memery: 7803\n",
      "(310) finished after 257 timesteps, total profit: -169.2779984507138, memery: 7803\n",
      "(311) finished after 1433 timesteps, total profit: -168.35577039472085, memery: 7807\n",
      "(312) finished after 1403 timesteps, total profit: -163.64477680078932, memery: 7807\n",
      "(313) finished after 1364 timesteps, total profit: -156.52406717306405, memery: 7808\n",
      "(314) finished after 521 timesteps, total profit: -166.49025897589075, memery: 7808\n",
      "(315) finished after 1230 timesteps, total profit: -159.52765774709508, memery: 7808\n",
      "(316) finished after 1239 timesteps, total profit: -160.63429641394583, memery: 7812\n",
      "(317) finished after 952 timesteps, total profit: -151.6849338826609, memery: 7812\n",
      "(318) finished after 969 timesteps, total profit: -163.65382320999257, memery: 7813\n",
      "(319) finished after 1289 timesteps, total profit: -166.0913873090459, memery: 7815\n",
      "(320) finished after 1203 timesteps, total profit: -163.56558619258783, memery: 7816\n",
      "(321) finished after 124 timesteps, total profit: -163.8940449132748, memery: 7816\n",
      "(322) finished after 1394 timesteps, total profit: -158.47515000297201, memery: 7820\n",
      "(323) finished after 175 timesteps, total profit: -156.85747429869087, memery: 7820\n",
      "(324) finished after 1383 timesteps, total profit: -167.8745489566608, memery: 7821\n",
      "(325) finished after 1363 timesteps, total profit: -173.6747826767406, memery: 7824\n",
      "(326) finished after 703 timesteps, total profit: -173.52108036703848, memery: 7824\n",
      "(327) finished after 796 timesteps, total profit: -150.5950008764394, memery: 7824\n",
      "(328) finished after 464 timesteps, total profit: -171.17170474631033, memery: 7824\n",
      "(329) finished after 387 timesteps, total profit: -164.75696048980618, memery: 7824\n",
      "(330) finished after 551 timesteps, total profit: -161.51118277180228, memery: 7824\n",
      "(331) finished after 1117 timesteps, total profit: -167.62292829871103, memery: 7824\n",
      "(332) finished after 279 timesteps, total profit: -151.71610989872426, memery: 7824\n",
      "(333) finished after 1202 timesteps, total profit: -176.59282118082595, memery: 7824\n",
      "(334) finished after 985 timesteps, total profit: -154.78080096771922, memery: 7825\n",
      "(335) finished after 202 timesteps, total profit: -167.92012366748486, memery: 7825\n",
      "(336) finished after 684 timesteps, total profit: -157.35338153200408, memery: 7825\n",
      "(337) finished after 1204 timesteps, total profit: -150.66462296988897, memery: 7828\n",
      "(338) finished after 757 timesteps, total profit: -153.22818227978416, memery: 7828\n",
      "(339) finished after 1446 timesteps, total profit: -153.99370207530396, memery: 7829\n",
      "(340) finished after 184 timesteps, total profit: -156.29532181351263, memery: 7829\n",
      "(341) finished after 1409 timesteps, total profit: -163.76187202493608, memery: 7830\n",
      "(342) finished after 1366 timesteps, total profit: -151.4591522342872, memery: 7833\n",
      "(343) finished after 1434 timesteps, total profit: -160.64647708099906, memery: 7839\n",
      "(344) finished after 1308 timesteps, total profit: -156.09352391009116, memery: 7840\n",
      "(345) finished after 1374 timesteps, total profit: -166.62408868843772, memery: 7840\n",
      "(346) finished after 1394 timesteps, total profit: -209.46206876544738, memery: 7843\n",
      "(347) finished after 1367 timesteps, total profit: -178.57667395030114, memery: 7844\n",
      "(348) finished after 1477 timesteps, total profit: -165.55412050105568, memery: 7846\n",
      "(349) finished after 1648 timesteps, total profit: -174.50118757554822, memery: 7862\n",
      "(350) finished after 1366 timesteps, total profit: -150.1166787946021, memery: 7863\n",
      "(351) finished after 985 timesteps, total profit: -163.52689131795532, memery: 7864\n",
      "(352) finished after 1365 timesteps, total profit: -153.0076888059352, memery: 7865\n",
      "(353) finished after 1040 timesteps, total profit: -150.07067380988514, memery: 7865\n",
      "(354) finished after 1400 timesteps, total profit: -172.64937736334093, memery: 7870\n",
      "(355) finished after 1450 timesteps, total profit: -176.58746910172417, memery: 7872\n",
      "(356) finished after 1202 timesteps, total profit: -165.89158142779627, memery: 7873\n",
      "(357) finished after 1348 timesteps, total profit: -150.54479496043484, memery: 7873\n",
      "(358) finished after 1413 timesteps, total profit: -154.33875654497422, memery: 7874\n",
      "(359) finished after 1774 timesteps, total profit: -150.76017731164606, memery: 7875\n",
      "(360) finished after 1412 timesteps, total profit: -150.50034112285897, memery: 7875\n",
      "(361) finished after 872 timesteps, total profit: -152.50875767458314, memery: 7876\n",
      "(362) finished after 1281 timesteps, total profit: -151.0124146148333, memery: 7876\n",
      "(363) finished after 479 timesteps, total profit: -172.45555954697224, memery: 7876\n",
      "(364) finished after 217 timesteps, total profit: -162.2860229977597, memery: 7876\n",
      "(365) finished after 1547 timesteps, total profit: -156.38298972603684, memery: 7876\n",
      "(366) finished after 400 timesteps, total profit: -160.38304811084012, memery: 7876\n",
      "(367) finished after 411 timesteps, total profit: -150.61087682889726, memery: 7876\n",
      "(368) finished after 1346 timesteps, total profit: -155.5697479376327, memery: 7877\n",
      "(369) finished after 880 timesteps, total profit: -156.43200297306146, memery: 7877\n",
      "(370) finished after 606 timesteps, total profit: -152.19045495204418, memery: 7877\n",
      "(371) finished after 1301 timesteps, total profit: -163.69677806118938, memery: 7877\n",
      "(372) finished after 1330 timesteps, total profit: -156.91753709878066, memery: 7878\n",
      "(373) finished after 1238 timesteps, total profit: -151.60724541855637, memery: 7879\n",
      "(374) finished after 1201 timesteps, total profit: -252.7597325852882, memery: 7880\n",
      "(375) finished after 377 timesteps, total profit: -163.63101337967385, memery: 7880\n",
      "(376) finished after 18410 timesteps, total profit: -185.76692379654713, memery: 8772\n",
      "(377) finished after 1113 timesteps, total profit: -150.7034103192062, memery: 8775\n",
      "(378) finished after 795 timesteps, total profit: -166.05064592165854, memery: 8776\n",
      "(379) finished after 364 timesteps, total profit: -151.90878768504945, memery: 8776\n",
      "(380) finished after 1255 timesteps, total profit: -191.1159355234395, memery: 8779\n",
      "(381) finished after 1369 timesteps, total profit: -179.74639234065813, memery: 8783\n",
      "(382) finished after 209 timesteps, total profit: -150.3479846898681, memery: 8783\n",
      "(383) finished after 780 timesteps, total profit: -152.93447670620318, memery: 8784\n",
      "(384) finished after 1424 timesteps, total profit: -204.66593572220563, memery: 8790\n",
      "(385) finished after 1497 timesteps, total profit: -169.28614153038632, memery: 8798\n",
      "(386) finished after 1389 timesteps, total profit: -159.2642315185545, memery: 8800\n",
      "(387) finished after 237 timesteps, total profit: -157.14410802205146, memery: 8800\n",
      "(388) finished after 453 timesteps, total profit: -151.24377186806402, memery: 8800\n",
      "(389) finished after 514 timesteps, total profit: -156.648082851228, memery: 8801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(390) finished after 1283 timesteps, total profit: -154.16915855058016, memery: 8803\n",
      "(391) finished after 709 timesteps, total profit: -161.20341465497486, memery: 8804\n",
      "(392) finished after 1465 timesteps, total profit: -154.19345353260678, memery: 8805\n",
      "(393) finished after 1331 timesteps, total profit: -161.16811446942572, memery: 8806\n",
      "(394) finished after 1119 timesteps, total profit: -168.38857703863312, memery: 8806\n",
      "(395) finished after 487 timesteps, total profit: -154.98722759253522, memery: 8806\n",
      "(396) finished after 278 timesteps, total profit: -160.61770913887915, memery: 8806\n",
      "(397) finished after 648 timesteps, total profit: -166.42693886621663, memery: 8806\n",
      "(398) finished after 591 timesteps, total profit: -157.48161347575433, memery: 8806\n",
      "(399) finished after 1108 timesteps, total profit: -157.03663971535138, memery: 8806\n",
      "(400) finished after 191 timesteps, total profit: -169.01553787946926, memery: 8806\n",
      "(401) finished after 87 timesteps, total profit: -157.31025136249113, memery: 8806\n",
      "(402) finished after 204 timesteps, total profit: -162.54519369042026, memery: 8806\n",
      "(403) finished after 974 timesteps, total profit: -157.68579298851438, memery: 8807\n",
      "(404) finished after 1435 timesteps, total profit: -150.21949480441526, memery: 8810\n",
      "(405) finished after 990 timesteps, total profit: -171.63971856419786, memery: 8811\n",
      "(406) finished after 457 timesteps, total profit: -153.2330417041655, memery: 8811\n",
      "(407) finished after 198 timesteps, total profit: -170.17418594585183, memery: 8811\n",
      "(408) finished after 188 timesteps, total profit: -163.88692736497993, memery: 8811\n",
      "(409) finished after 1366 timesteps, total profit: -150.03239493371785, memery: 8812\n",
      "(410) finished after 1476 timesteps, total profit: -169.0060062682333, memery: 8873\n",
      "(411) finished after 179 timesteps, total profit: -157.41501855971734, memery: 8873\n",
      "(412) finished after 500 timesteps, total profit: -150.60881966704255, memery: 8874\n",
      "(413) finished after 1317 timesteps, total profit: -150.24906244558207, memery: 8875\n",
      "(414) finished after 708 timesteps, total profit: -159.5793785978121, memery: 8876\n",
      "(415) finished after 779 timesteps, total profit: -150.23542331316048, memery: 8877\n",
      "(416) finished after 725 timesteps, total profit: -151.635382812101, memery: 8877\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from trading_gym.env import TradeEnv\n",
    "from datetime import datetime\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "#matplotlib.use('tkAgg')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device =  torch.device(\"cpu\")\n",
    "\n",
    "def custom_obs_features_func(history, info):\n",
    "    list = []\n",
    "    for i in range(len(history.obs_list)):\n",
    "        list.append(history.obs_list[i].close)\n",
    "    \n",
    "    return list\n",
    "\n",
    "def custom_reward_func(exchange):\n",
    "    #info\n",
    "    '''\n",
    "    {'index': 56, 'date': '2010-01-01 01:04', 'nav': 50000, 'amount': 250000, 'avg_price': 1.4325899999999998,\n",
    "    'profit': {'total': -282.0124161115024, 'fixed': -272.23990618194, 'floating': -9.7725099295624},\n",
    "    'buy_at': 52, 'latest_price': 1.43231}\n",
    "    '''\n",
    "    #print(exchange.info)\n",
    "    #print('Profit: {} , floating: {} , fixed: {}'.format(exchange.profit, exchange.floating_profit, exchange.fixed_profit))\n",
    "    \n",
    "    # profit , index - 50\n",
    "    reward = exchange.floating_profit * (exchange.info[\"index\"] - 50) * 0.01\n",
    "    #print(exchange.info[\"amount\"])\n",
    "    #print(exchange.available_actions)\n",
    "    \n",
    "    return reward\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 5000\n",
    "env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-3.csv',\n",
    "               ops_shape=[],\n",
    "               get_obs_features_func=custom_obs_features_func,\n",
    "               get_reward_func=custom_reward_func,\n",
    "               nav=5000, \n",
    "               data_kwargs={'use_ta': False}\n",
    "              )\n",
    "\n",
    "#nv = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',ops_shape=[],get_obs_features_func=custom_obs_features_func,nav=1000,get_reward_func=custom_reward_func, data_kwargs={'use_ta': False})\n",
    "#env = TradeEnv(data_path='eurusd_patterns_10_test2_slope_trend_pro.lite1-6.csv',nav=1000, data_kwargs={'use_ta': False})\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = 3\n",
    "N_STATES = 51\n",
    "ENV_A_SHAPE = 0\n",
    "n_episodes = 10000 #10000\n",
    "PATH = \"./training_game_01.h5\"\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_STATES, 50).to(device)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.fc2 = nn.Linear(50, 50).to(device)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)   # initialization\n",
    "        self.out = nn.Linear(50, 50).to(device)\n",
    "        self.out.weight.data.normal_(0, 0.1)   # initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x)\n",
    "        #print(x.size())\n",
    "        x = x.to(device)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.size())\n",
    "        actions_value = self.out(x)\n",
    "        #print(actions_value)\n",
    "        #print(actions_value.size())\n",
    "        return actions_value\n",
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        self.eval_net, self.target_net = Net().to(device), Net().to(device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.memory_counter = 0                                         # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))     # initialize memory\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "    def choose_action(self, x):\n",
    "        #print(x)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0).to(device)\n",
    "        # input only one sample\n",
    "        if np.random.uniform() < EPSILON:   # greedy\n",
    "            actions_value = self.eval_net.forward(x)\n",
    "            action = torch.max(actions_value, 1)[1].data.cpu().numpy()\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)  # return the argmax index\n",
    "            \n",
    "        else:   # random\n",
    "            action = np.random.randint(0, N_ACTIONS)\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # target parameter update\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # sample batch transitions\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_index, :]\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES]).to(device)\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int)).to(device)\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2]).to(device)\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:]).to(device)\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)  # shape (batch, 1)\n",
    "        q_next = self.target_net(b_s_).detach()     # detach from graph, don't backpropagate\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)   # shape (batch, 1)\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "\n",
    "dqn = DQN()\n",
    "\n",
    "\n",
    "print('\\nCollecting experience...')\n",
    "for i_episode in range(n_episodes):\n",
    "    t = 0\n",
    "    rewards = 0\n",
    "    obs0 = env.reset()\n",
    "    while True:\n",
    "        #env.render()\n",
    "        action = dqn.choose_action(obs0)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        #action 0 sell, 1 hold, 2 buy\n",
    "\n",
    "        # 儲存 experience\n",
    "        if(info[\"profit\"][\"floating\"] > 20):\n",
    "            dqn.store_transition(obs0, action, reward, observation)\n",
    "        \n",
    "        # 累積 reward\n",
    "        rewards += reward\n",
    "\n",
    "        # 有足夠 experience 後進行訓練\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            #print(dqn.memory_counter)\n",
    "\n",
    "        # 進入下一 state\n",
    "        obs0 = observation\n",
    "\n",
    "        if(done):\n",
    "            #torch.save(dqn, PATH)\n",
    "            print('({}) finished after {} timesteps, total profit: {}, memery: {}'.format(i_episode+1, t+1, info[\"profit\"][\"total\"],dqn.memory_counter))\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
